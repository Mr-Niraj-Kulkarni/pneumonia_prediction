{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the modules required \n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms,models \n",
    "from glob import glob\n",
    "import cv2\n",
    "import os \n",
    "from PIL import Image\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use transform  to make changes to the images we need load for classification\n",
    "\n",
    "\n",
    "'''\n",
    "transform = transforms.Compose([\n",
    " transforms.Resize((256,256)),                \n",
    " transforms.ToTensor(),   \n",
    " transforms.Normalize(                                                \n",
    " mean=[0.485, 0.456, 0.406],                \n",
    " std=[0.229, 0.224, 0.225]                  \n",
    " )])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify path to the train,val and test folder \n",
    "path_train=\"chest_xray/train\"\n",
    "path_validation=\"chest_xray/val\"\n",
    "path_test=\"chest_xray/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loading images for classification  we create folder for each class images n using dataset.\n",
    "ImageFolder the labels are directly assigned.\n",
    "\n",
    "\"\"\"\n",
    "train_data = datasets.ImageFolder(path_train,       \n",
    "                    transform=transform)\n",
    "validation_data = datasets.ImageFolder(path_validation,       \n",
    "                    transform=transform)\n",
    "test_data=datasets.ImageFolder(path_test,       \n",
    "                    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader function to load the data in pytorch format  \n",
    "trainloader=torch.utils.data.DataLoader(dataset=train_data,shuffle=True,batch_size=32)\n",
    "validationloader=torch.utils.data.DataLoader(dataset=validation_data,shuffle=True,batch_size=32)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_data,shuffle=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laod the resnet34 model with pretrained weights\n",
    "model = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use this no to train the model parameters again\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#add the last linear layer for custom classification     \n",
    "model.fc = nn.Sequential(nn.Linear(512, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 2),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "#set the loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#set the optimizer to be used \n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.005)\n",
    "\n",
    "#model to device  \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10.. Train loss: 0.281.. Test loss: 0.455.. Test accuracy: 0.625\n",
      "Epoch 1/10.. Train loss: 0.197.. Test loss: 1.233.. Test accuracy: 0.500\n",
      "Epoch 1/10.. Train loss: 0.203.. Test loss: 1.051.. Test accuracy: 0.562\n",
      "Epoch 1/10.. Train loss: 0.212.. Test loss: 0.476.. Test accuracy: 0.625\n",
      "Epoch 1/10.. Train loss: 0.241.. Test loss: 0.336.. Test accuracy: 0.750\n",
      "Epoch 1/10.. Train loss: 0.295.. Test loss: 0.296.. Test accuracy: 0.938\n",
      "Epoch 1/10.. Train loss: 0.217.. Test loss: 0.789.. Test accuracy: 0.562\n",
      "Epoch 1/10.. Train loss: 0.188.. Test loss: 0.957.. Test accuracy: 0.562\n",
      "Epoch 1/10.. Train loss: 0.136.. Test loss: 0.230.. Test accuracy: 0.938\n",
      "Epoch 1/10.. Train loss: 0.196.. Test loss: 0.354.. Test accuracy: 0.750\n",
      "Epoch 1/10.. Train loss: 0.186.. Test loss: 0.308.. Test accuracy: 0.750\n",
      "Epoch 1/10.. Train loss: 0.147.. Test loss: 0.678.. Test accuracy: 0.625\n",
      "Epoch 1/10.. Train loss: 0.161.. Test loss: 0.609.. Test accuracy: 0.625\n",
      "Epoch 1/10.. Train loss: 0.161.. Test loss: 0.326.. Test accuracy: 0.750\n",
      "Epoch 1/10.. Train loss: 0.141.. Test loss: 0.190.. Test accuracy: 0.938\n",
      "Epoch 1/10.. Train loss: 0.180.. Test loss: 0.239.. Test accuracy: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\niraj\\Anaconda3\\envs\\yourenvname\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10.. Train loss: 0.149.. Test loss: 1.295.. Test accuracy: 0.562\n",
      "Epoch 2/10.. Train loss: 0.256.. Test loss: 0.657.. Test accuracy: 0.562\n",
      "Epoch 2/10.. Train loss: 0.174.. Test loss: 0.216.. Test accuracy: 0.938\n",
      "Epoch 2/10.. Train loss: 0.145.. Test loss: 0.874.. Test accuracy: 0.562\n",
      "Epoch 2/10.. Train loss: 0.195.. Test loss: 0.704.. Test accuracy: 0.562\n",
      "Epoch 2/10.. Train loss: 0.158.. Test loss: 0.385.. Test accuracy: 0.812\n",
      "Epoch 2/10.. Train loss: 0.126.. Test loss: 0.311.. Test accuracy: 0.750\n",
      "Epoch 2/10.. Train loss: 0.129.. Test loss: 0.297.. Test accuracy: 0.812\n",
      "Epoch 2/10.. Train loss: 0.130.. Test loss: 0.183.. Test accuracy: 0.938\n",
      "Epoch 2/10.. Train loss: 0.155.. Test loss: 1.167.. Test accuracy: 0.562\n",
      "Epoch 2/10.. Train loss: 0.165.. Test loss: 0.688.. Test accuracy: 0.562\n",
      "Epoch 2/10.. Train loss: 0.118.. Test loss: 0.181.. Test accuracy: 0.938\n",
      "Epoch 2/10.. Train loss: 0.156.. Test loss: 0.609.. Test accuracy: 0.625\n",
      "Epoch 2/10.. Train loss: 0.146.. Test loss: 0.951.. Test accuracy: 0.562\n",
      "Epoch 2/10.. Train loss: 0.181.. Test loss: 0.288.. Test accuracy: 0.812\n",
      "Epoch 2/10.. Train loss: 0.194.. Test loss: 0.229.. Test accuracy: 0.938\n",
      "Epoch 3/10.. Train loss: 0.106.. Test loss: 0.275.. Test accuracy: 0.875\n",
      "Epoch 3/10.. Train loss: 0.187.. Test loss: 1.135.. Test accuracy: 0.562\n",
      "Epoch 3/10.. Train loss: 0.076.. Test loss: 0.235.. Test accuracy: 0.875\n",
      "Epoch 3/10.. Train loss: 0.096.. Test loss: 0.487.. Test accuracy: 0.688\n",
      "Epoch 3/10.. Train loss: 0.121.. Test loss: 0.390.. Test accuracy: 0.750\n",
      "Epoch 3/10.. Train loss: 0.105.. Test loss: 0.173.. Test accuracy: 0.938\n",
      "Epoch 3/10.. Train loss: 0.146.. Test loss: 0.683.. Test accuracy: 0.688\n",
      "Epoch 3/10.. Train loss: 0.179.. Test loss: 0.307.. Test accuracy: 0.812\n",
      "Epoch 3/10.. Train loss: 0.179.. Test loss: 0.200.. Test accuracy: 0.938\n",
      "Epoch 3/10.. Train loss: 0.233.. Test loss: 0.339.. Test accuracy: 0.688\n",
      "Epoch 3/10.. Train loss: 0.219.. Test loss: 0.811.. Test accuracy: 0.562\n",
      "Epoch 3/10.. Train loss: 0.120.. Test loss: 0.751.. Test accuracy: 0.562\n",
      "Epoch 3/10.. Train loss: 0.170.. Test loss: 0.183.. Test accuracy: 0.938\n",
      "Epoch 3/10.. Train loss: 0.158.. Test loss: 0.777.. Test accuracy: 0.562\n",
      "Epoch 3/10.. Train loss: 0.099.. Test loss: 0.140.. Test accuracy: 0.938\n",
      "Epoch 3/10.. Train loss: 0.132.. Test loss: 0.412.. Test accuracy: 0.688\n",
      "Epoch 4/10.. Train loss: 0.178.. Test loss: 0.456.. Test accuracy: 0.688\n",
      "Epoch 4/10.. Train loss: 0.105.. Test loss: 0.298.. Test accuracy: 0.812\n",
      "Epoch 4/10.. Train loss: 0.106.. Test loss: 0.686.. Test accuracy: 0.625\n",
      "Epoch 4/10.. Train loss: 0.096.. Test loss: 0.144.. Test accuracy: 0.938\n",
      "Epoch 4/10.. Train loss: 0.105.. Test loss: 0.793.. Test accuracy: 0.562\n",
      "Epoch 4/10.. Train loss: 0.140.. Test loss: 0.114.. Test accuracy: 0.938\n",
      "Epoch 4/10.. Train loss: 0.134.. Test loss: 0.250.. Test accuracy: 0.875\n",
      "Epoch 4/10.. Train loss: 0.117.. Test loss: 0.825.. Test accuracy: 0.562\n",
      "Epoch 4/10.. Train loss: 0.152.. Test loss: 0.191.. Test accuracy: 0.938\n",
      "Epoch 4/10.. Train loss: 0.175.. Test loss: 0.185.. Test accuracy: 0.938\n",
      "Epoch 4/10.. Train loss: 0.156.. Test loss: 0.283.. Test accuracy: 0.812\n",
      "Epoch 4/10.. Train loss: 0.204.. Test loss: 1.072.. Test accuracy: 0.562\n",
      "Epoch 4/10.. Train loss: 0.215.. Test loss: 0.377.. Test accuracy: 0.750\n",
      "Epoch 4/10.. Train loss: 0.149.. Test loss: 0.138.. Test accuracy: 0.938\n",
      "Epoch 4/10.. Train loss: 0.169.. Test loss: 0.151.. Test accuracy: 0.938\n",
      "Epoch 4/10.. Train loss: 0.182.. Test loss: 0.551.. Test accuracy: 0.688\n",
      "Epoch 4/10.. Train loss: 0.098.. Test loss: 0.153.. Test accuracy: 1.000\n",
      "Epoch 5/10.. Train loss: 0.116.. Test loss: 0.384.. Test accuracy: 0.750\n",
      "Epoch 5/10.. Train loss: 0.136.. Test loss: 0.199.. Test accuracy: 0.938\n",
      "Epoch 5/10.. Train loss: 0.117.. Test loss: 0.359.. Test accuracy: 0.750\n",
      "Epoch 5/10.. Train loss: 0.190.. Test loss: 1.236.. Test accuracy: 0.562\n",
      "Epoch 5/10.. Train loss: 0.233.. Test loss: 0.207.. Test accuracy: 0.938\n",
      "Epoch 5/10.. Train loss: 0.177.. Test loss: 0.262.. Test accuracy: 0.938\n",
      "Epoch 5/10.. Train loss: 0.137.. Test loss: 0.421.. Test accuracy: 0.688\n",
      "Epoch 5/10.. Train loss: 0.190.. Test loss: 0.592.. Test accuracy: 0.688\n",
      "Epoch 5/10.. Train loss: 0.140.. Test loss: 0.330.. Test accuracy: 0.812\n",
      "Epoch 5/10.. Train loss: 0.157.. Test loss: 0.158.. Test accuracy: 0.938\n",
      "Epoch 5/10.. Train loss: 0.112.. Test loss: 0.828.. Test accuracy: 0.562\n",
      "Epoch 5/10.. Train loss: 0.122.. Test loss: 0.467.. Test accuracy: 0.750\n",
      "Epoch 5/10.. Train loss: 0.103.. Test loss: 0.143.. Test accuracy: 0.938\n",
      "Epoch 5/10.. Train loss: 0.091.. Test loss: 0.367.. Test accuracy: 0.812\n",
      "Epoch 5/10.. Train loss: 0.123.. Test loss: 0.150.. Test accuracy: 0.938\n",
      "Epoch 5/10.. Train loss: 0.168.. Test loss: 0.298.. Test accuracy: 0.750\n",
      "Epoch 6/10.. Train loss: 0.156.. Test loss: 0.792.. Test accuracy: 0.562\n",
      "Epoch 6/10.. Train loss: 0.196.. Test loss: 0.210.. Test accuracy: 0.938\n",
      "Epoch 6/10.. Train loss: 0.190.. Test loss: 0.194.. Test accuracy: 0.938\n",
      "Epoch 6/10.. Train loss: 0.168.. Test loss: 0.376.. Test accuracy: 0.812\n",
      "Epoch 6/10.. Train loss: 0.164.. Test loss: 0.273.. Test accuracy: 0.812\n",
      "Epoch 6/10.. Train loss: 0.133.. Test loss: 0.122.. Test accuracy: 1.000\n",
      "Epoch 6/10.. Train loss: 0.146.. Test loss: 0.595.. Test accuracy: 0.625\n",
      "Epoch 6/10.. Train loss: 0.128.. Test loss: 0.097.. Test accuracy: 0.938\n",
      "Epoch 6/10.. Train loss: 0.177.. Test loss: 0.443.. Test accuracy: 0.688\n",
      "Epoch 6/10.. Train loss: 0.118.. Test loss: 0.133.. Test accuracy: 1.000\n",
      "Epoch 6/10.. Train loss: 0.176.. Test loss: 0.301.. Test accuracy: 0.812\n",
      "Epoch 6/10.. Train loss: 0.209.. Test loss: 0.274.. Test accuracy: 0.875\n",
      "Epoch 6/10.. Train loss: 0.093.. Test loss: 0.413.. Test accuracy: 0.750\n",
      "Epoch 6/10.. Train loss: 0.095.. Test loss: 0.274.. Test accuracy: 0.812\n",
      "Epoch 6/10.. Train loss: 0.132.. Test loss: 0.244.. Test accuracy: 0.812\n",
      "Epoch 6/10.. Train loss: 0.122.. Test loss: 1.179.. Test accuracy: 0.562\n",
      "Epoch 7/10.. Train loss: 0.107.. Test loss: 0.149.. Test accuracy: 1.000\n",
      "Epoch 7/10.. Train loss: 0.075.. Test loss: 0.123.. Test accuracy: 1.000\n",
      "Epoch 7/10.. Train loss: 0.154.. Test loss: 0.266.. Test accuracy: 0.812\n",
      "Epoch 7/10.. Train loss: 0.135.. Test loss: 0.183.. Test accuracy: 1.000\n",
      "Epoch 7/10.. Train loss: 0.110.. Test loss: 0.816.. Test accuracy: 0.562\n",
      "Epoch 7/10.. Train loss: 0.125.. Test loss: 0.147.. Test accuracy: 0.938\n",
      "Epoch 7/10.. Train loss: 0.198.. Test loss: 0.280.. Test accuracy: 0.812\n",
      "Epoch 7/10.. Train loss: 0.092.. Test loss: 0.202.. Test accuracy: 0.875\n",
      "Epoch 7/10.. Train loss: 0.170.. Test loss: 0.146.. Test accuracy: 1.000\n",
      "Epoch 7/10.. Train loss: 0.183.. Test loss: 0.671.. Test accuracy: 0.562\n",
      "Epoch 7/10.. Train loss: 0.109.. Test loss: 0.304.. Test accuracy: 0.812\n",
      "Epoch 7/10.. Train loss: 0.158.. Test loss: 0.193.. Test accuracy: 0.938\n",
      "Epoch 7/10.. Train loss: 0.097.. Test loss: 0.334.. Test accuracy: 0.812\n",
      "Epoch 7/10.. Train loss: 0.141.. Test loss: 0.315.. Test accuracy: 0.750\n",
      "Epoch 7/10.. Train loss: 0.124.. Test loss: 0.218.. Test accuracy: 0.938\n",
      "Epoch 7/10.. Train loss: 0.122.. Test loss: 0.325.. Test accuracy: 0.812\n",
      "Epoch 7/10.. Train loss: 0.157.. Test loss: 0.300.. Test accuracy: 0.812\n",
      "Epoch 8/10.. Train loss: 0.114.. Test loss: 0.280.. Test accuracy: 0.875\n",
      "Epoch 8/10.. Train loss: 0.070.. Test loss: 0.232.. Test accuracy: 0.875\n",
      "Epoch 8/10.. Train loss: 0.148.. Test loss: 0.298.. Test accuracy: 0.812\n",
      "Epoch 8/10.. Train loss: 0.111.. Test loss: 0.196.. Test accuracy: 1.000\n",
      "Epoch 8/10.. Train loss: 0.143.. Test loss: 0.388.. Test accuracy: 0.812\n",
      "Epoch 8/10.. Train loss: 0.115.. Test loss: 0.142.. Test accuracy: 0.938\n",
      "Epoch 8/10.. Train loss: 0.137.. Test loss: 0.502.. Test accuracy: 0.688\n",
      "Epoch 8/10.. Train loss: 0.091.. Test loss: 0.148.. Test accuracy: 0.938\n",
      "Epoch 8/10.. Train loss: 0.078.. Test loss: 0.761.. Test accuracy: 0.562\n",
      "Epoch 8/10.. Train loss: 0.171.. Test loss: 0.333.. Test accuracy: 0.812\n",
      "Epoch 8/10.. Train loss: 0.121.. Test loss: 0.211.. Test accuracy: 0.938\n",
      "Epoch 8/10.. Train loss: 0.114.. Test loss: 0.637.. Test accuracy: 0.688\n",
      "Epoch 8/10.. Train loss: 0.170.. Test loss: 0.164.. Test accuracy: 1.000\n",
      "Epoch 8/10.. Train loss: 0.145.. Test loss: 0.374.. Test accuracy: 0.812\n",
      "Epoch 8/10.. Train loss: 0.116.. Test loss: 0.199.. Test accuracy: 0.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10.. Train loss: 0.105.. Test loss: 0.306.. Test accuracy: 0.812\n",
      "Epoch 9/10.. Train loss: 0.079.. Test loss: 0.272.. Test accuracy: 0.812\n",
      "Epoch 9/10.. Train loss: 0.076.. Test loss: 0.101.. Test accuracy: 1.000\n",
      "Epoch 9/10.. Train loss: 0.146.. Test loss: 0.259.. Test accuracy: 0.812\n",
      "Epoch 9/10.. Train loss: 0.134.. Test loss: 0.343.. Test accuracy: 0.812\n",
      "Epoch 9/10.. Train loss: 0.111.. Test loss: 0.332.. Test accuracy: 0.812\n",
      "Epoch 9/10.. Train loss: 0.173.. Test loss: 0.197.. Test accuracy: 0.938\n",
      "Epoch 9/10.. Train loss: 0.107.. Test loss: 0.299.. Test accuracy: 0.750\n",
      "Epoch 9/10.. Train loss: 0.134.. Test loss: 0.100.. Test accuracy: 1.000\n",
      "Epoch 9/10.. Train loss: 0.102.. Test loss: 0.269.. Test accuracy: 0.875\n",
      "Epoch 9/10.. Train loss: 0.090.. Test loss: 0.131.. Test accuracy: 1.000\n",
      "Epoch 9/10.. Train loss: 0.079.. Test loss: 0.365.. Test accuracy: 0.750\n",
      "Epoch 9/10.. Train loss: 0.080.. Test loss: 0.157.. Test accuracy: 0.938\n",
      "Epoch 9/10.. Train loss: 0.193.. Test loss: 0.259.. Test accuracy: 0.812\n",
      "Epoch 9/10.. Train loss: 0.169.. Test loss: 0.170.. Test accuracy: 1.000\n",
      "Epoch 9/10.. Train loss: 0.144.. Test loss: 0.100.. Test accuracy: 1.000\n",
      "Epoch 9/10.. Train loss: 0.142.. Test loss: 0.154.. Test accuracy: 1.000\n",
      "Epoch 10/10.. Train loss: 0.100.. Test loss: 0.442.. Test accuracy: 0.750\n",
      "Epoch 10/10.. Train loss: 0.133.. Test loss: 0.521.. Test accuracy: 0.750\n",
      "Epoch 10/10.. Train loss: 0.084.. Test loss: 0.104.. Test accuracy: 1.000\n",
      "Epoch 10/10.. Train loss: 0.164.. Test loss: 0.305.. Test accuracy: 0.875\n",
      "Epoch 10/10.. Train loss: 0.079.. Test loss: 0.422.. Test accuracy: 0.750\n",
      "Epoch 10/10.. Train loss: 0.119.. Test loss: 0.233.. Test accuracy: 0.875\n",
      "Epoch 10/10.. Train loss: 0.100.. Test loss: 0.097.. Test accuracy: 1.000\n",
      "Epoch 10/10.. Train loss: 0.122.. Test loss: 0.382.. Test accuracy: 0.750\n",
      "Epoch 10/10.. Train loss: 0.129.. Test loss: 0.132.. Test accuracy: 1.000\n",
      "Epoch 10/10.. Train loss: 0.098.. Test loss: 0.427.. Test accuracy: 0.812\n",
      "Epoch 10/10.. Train loss: 0.087.. Test loss: 0.147.. Test accuracy: 0.938\n",
      "Epoch 10/10.. Train loss: 0.131.. Test loss: 0.915.. Test accuracy: 0.562\n",
      "Epoch 10/10.. Train loss: 0.133.. Test loss: 0.204.. Test accuracy: 0.938\n",
      "Epoch 10/10.. Train loss: 0.087.. Test loss: 0.203.. Test accuracy: 0.875\n",
      "Epoch 10/10.. Train loss: 0.090.. Test loss: 0.065.. Test accuracy: 1.000\n",
      "Epoch 10/10.. Train loss: 0.162.. Test loss: 0.237.. Test accuracy: 0.875\n",
      "Epoch 10/10.. Train loss: 0.104.. Test loss: 0.134.. Test accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10    #number of epochs \n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10     #used for printing different verbose \n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:     \n",
    "        steps += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()     #initiate optimizer \n",
    "        logps = model.forward(inputs) \n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in validationloader:\n",
    "                    inputs, labels = inputs.to(device),labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy +=torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(validationloader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(validationloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(validationloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "    torch.save(model, 'models/pneumonia_prediction_1 '+str(epoch)+' .pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 85.0\n"
     ]
    }
   ],
   "source": [
    "#testing the model on test dataset available \n",
    "model1 = torch.load(\"models/pneumonia_prediction_1 9 .pth\")\n",
    "accuracy=0 \n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        # evaluate the model on the test set\n",
    "        inputs, labels = inputs.to(device),targets.to(device)\n",
    "        yhat = model1.forward(inputs)\n",
    "        ps = torch.exp(yhat)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy +=torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "print(f\"Test accuracy: {(accuracy/len(test_loader) )*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
